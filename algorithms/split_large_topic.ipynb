{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# path\n",
    "parser.add_argument(\"--root_data_dir\",type=str,default=\"/home/featurize/CB4Rec/data/\")\n",
    "parser.add_argument(\"--root_proj_dir\",type=str,default=\"/home/featurize/CB4Rec/\")\n",
    "# parser.add_argument(\"--root_proj_dir\",type=str,default=\"./\")\n",
    "# parser.add_argument(\"--model_path\", type=str, default=\"/home/v-zhenyuhe/CB4Rec/model/large/large.pkl\")\n",
    "parser.add_argument(\"--sim_path\", type=str, default=\"pretrained_models/sim_nrms_bce_r14_ep6_thres038414\")\n",
    "parser.add_argument(\"--sim_threshold\", type=float, default=0.38414)\n",
    "\n",
    "# Preprocessing \n",
    "parser.add_argument(\"--dataset\",type=str,default='large')\n",
    "parser.add_argument(\"--num_selected_users\", type=int, default=1000, help='number of randomly selected users from val set')\n",
    "parser.add_argument(\"--cb_train_ratio\", type=float, default=0.2)\n",
    "parser.add_argument(\"--sim_npratio\", type=int, default=4)\n",
    "parser.add_argument(\"--sim_val_batch_size\", type=int, default=1024)\n",
    "\n",
    "# Simulation\n",
    "parser.add_argument(\"--algo\",type=str,default=\"2_ts_neuralucb\")\n",
    "parser.add_argument(\"--algo_prefix\", type=str, default=\"algo\",\n",
    "    help='the name of save files')\n",
    "parser.add_argument(\"--n_trials\", type=int, default=4, help = 'number of experiment runs')\n",
    "parser.add_argument(\"--T\", type=int, default=1000, help = 'number of rounds (interactions)')\n",
    "parser.add_argument(\"--topic_update_period\", type=int, default=1, help = 'Update period for CB topic model')\n",
    "parser.add_argument(\"--update_period\", type=int, default=100, help = 'Update period for CB model')\n",
    "parser.add_argument(\"--n_inference\", type=int, default=5, help='number of Monte Carlo samples of prediction. ')\n",
    "parser.add_argument(\"--rec_batch_size\", type=int, default=5, help='recommendation size for each round.')\n",
    "parser.add_argument(\"--per_rec_score_budget\", type=int, default=1000, help='buget for calcuating scores, e.g. ucb, for each rec')\n",
    "parser.add_argument(\"--max_batch_size\", type=int, default=256, help = 'Maximum batch size your GPU can fit in.')\n",
    "parser.add_argument(\"--pretrained_mode\",type=bool,default=True, \n",
    "    help=\"Indicates whether to load a pretrained model. True: load from a pretrained model, False: no pretrained model \")\n",
    "parser.add_argument(\"--preinference_mode\",type=bool,default=True, \n",
    "    help=\"Indicates whether to preinference news before each model update.\")\n",
    "\n",
    "parser.add_argument(\"--uniform_init\",type=bool,default=True, \n",
    "    help=\"For Thompson Sampling: Indicates whether to init ts parameters uniformly\")\n",
    "parser.add_argument(\"--gamma\", type=float, default=1.0, help='ucb parameter: mean + gamma * std.')\n",
    "\n",
    "\n",
    "# nrms \n",
    "parser.add_argument(\"--npratio\", type=int, default=4) \n",
    "parser.add_argument(\"--max_his_len\", type=int, default=50)\n",
    "parser.add_argument(\"--min_word_cnt\", type=int, default=1) # 5\n",
    "parser.add_argument(\"--max_title_len\", type=int, default=30)\n",
    "# nrms topic\n",
    "parser.add_argument(\"--dynamic_aggregate_topic\", type=bool, default=True) # whether to dynamicly aggregate small topic during simulation\n",
    "parser.add_argument(\"--min_item_size\", type=int, default=1000)\n",
    "\n",
    "# model training\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64) \n",
    "parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def load_cb_topic_news(root_data_dir):\n",
    "    fname = os.path.join(root_data_dir, \"large/utils/cb_news.pkl\") \n",
    "    with open(fname, 'rb') as f: \n",
    "        cb_news = pickle.load(f)\n",
    "    return cb_news \n",
    "topic_news = load_cb_topic_news(\"/home/featurize/CB4Rec/data\")\n",
    "cb_news = defaultdict(list)\n",
    "for k,v in topic_news.items():\n",
    "    cb_news[k] = [l.strip('\\n').split(\"\\t\")[0] for l in v] # get nIDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "large_topic = []\n",
    "for topic, nids in cb_news.items():\n",
    "    if len(nids) > 2000:\n",
    "        large_topic.append(topic)\n",
    "        \n",
    "json.dump(large_topic, open(\"../data/large/utils/large_topic.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "root_data_dir = \"/home/featurize/CB4Rec/data\"\n",
    "dataset = \"large\"\n",
    "with open(os.path.join(\"/home/featurize/CB4Rec/data\", \"large\",  'utils', 'nid2index.pkl'), 'rb') as f:\n",
    "    nid2index = pickle.load(f)\n",
    "news_index = np.load(os.path.join(root_data_dir, dataset,  'utils', 'news_index.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:04<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "word2vec = np.load(os.path.join(args.root_data_dir, args.dataset,  'utils', 'embedding.npy'))\n",
    "cb_news_embedding = {}\n",
    "for topic in tqdm(large_topic):\n",
    "    cb_news_embedding[topic] = []\n",
    "    for nid in cb_news[topic]:\n",
    "        total = len(np.where(news_index[nid2index[nid]]!=0))\n",
    "        news_embedding = np.sum(word2vec[np.array(news_index[nid2index[nid]], dtype=int)], axis=0)\n",
    "        news_mebedding = news_embedding / total\n",
    "        cb_news_embedding[topic].append(news_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "large_topic_cluster = {}\n",
    "for topic in large_topic:\n",
    "    num_cluster = len(cb_news[topic]) // 2000 + 1\n",
    "    kmeans = KMeans(n_clusters=num_cluster, random_state=0).fit(cb_news_embedding[topic])\n",
    "    large_topic_cluster[topic] = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(large_topic_cluster, open(\"../data/large/utils/large_topic_cluster.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory_by_order = json.load(open(os.path.join(root_data_dir, dataset,  'utils', 'subcategory_byorder.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "nid2topic = {}\n",
    "for topic in large_topic:\n",
    "    subcategory_by_order.remove(topic)\n",
    "    for nid, sub in zip(cb_news[topic], large_topic_cluster[topic]):\n",
    "        new_topic = topic + \"_\" + str(sub)\n",
    "        nid2topic[nid] = new_topic\n",
    "        if new_topic not in subcategory_by_order:\n",
    "            subcategory_by_order.append(new_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(subcategory_by_order,open(os.path.join(root_data_dir, dataset,  'utils', 'subcategory_byorder_large_topic_splited.json'), 'r'))\n",
    "json.dump(nid2topic,open(os.path.join(root_data_dir, dataset,  'utils', 'nid2topic_large_topic_splited.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1941, 0: 915}\n",
      "--------------\n",
      "{0: 713, 1: 1867}\n",
      "--------------\n",
      "{0: 1575, 3: 716, 4: 1587, 5: 1466, 2: 2580, 6: 2378, 1: 1806}\n",
      "--------------\n",
      "{2: 710, 0: 1731, 1: 1894}\n",
      "--------------\n",
      "{1: 1790, 0: 911}\n",
      "--------------\n",
      "{2: 2239, 0: 837, 1: 2225}\n",
      "--------------\n",
      "{0: 1155, 1: 1723}\n",
      "--------------\n",
      "{0: 1050, 1: 1734}\n",
      "--------------\n",
      "{1: 2111, 0: 1422}\n",
      "--------------\n",
      "{7: 569, 0: 1485, 2: 793, 5: 2788, 1: 2152, 4: 2002, 6: 2027, 3: 3057}\n",
      "--------------\n",
      "{0: 1215, 1: 2073}\n",
      "--------------\n",
      "{0: 2367, 1: 1141}\n",
      "--------------\n",
      "{1: 2319, 0: 1363}\n",
      "--------------\n",
      "{1: 1394, 0: 2367}\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for topic in large_topic:\n",
    "    c = Counter(large_topic_cluster[topic])\n",
    "    print(dict(c))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subcategory_by_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b114295533213be714c497b6c7c7c36862ca698da8b4418201631177dea05d47"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
